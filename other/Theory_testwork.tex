\documentclass[12pt,a4paper]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{amsmath}
\usepackage{graphicx, amssymb, hyperref, placeins, cite}
\usepackage{float}
\hypersetup{hidelinks}
\usepackage{indentfirst}
\usepackage{array}

\begin{document}

\title{Конспект билетов по теории}
\author{}
\date{\today}

\maketitle
\tableofcontents
\newpage

\section{Корреляция Пирсона, доверительный интервал и тест; Спирмен, Кендалл}

\textbf{Пирсон} — сила \emph{линейной} связи для количественных признаков при приблизительной нормальности и эллиптическом облаке точек; чувствителен к выбросам. 
\[
r=\frac{\overline{XY}-\overline X\,\overline Y}{S_X S_Y},\quad
\]
Где $S^{2}_X$ и $S^{2}_Y$ — смещённые оценки дисперсий.

\textbf{Спирмен} – позволяет уйти от условия линейности зависимости, заменив наблюдения $X_i$ на их ранги $R_i$ в ряду $X$, а $Y_i$ на ранги $T_i$ в ряду $Y$.  
\[
    \rho_S = \frac{\overline{RT} - \overline{R} \overline{T}}{\sqrt{S^2_R S^2_T}},
\]

\textbf{Кендалл} – мера монотонной зависимости между двумя переменными. Основана на подсчёте согласованных и несогласованных пар наблюдений.
\[
    \tau = \frac{C-D}{C+D} = \frac{2(C-D)}{n(n-1)},
\]
где $C$ — число согласованных пар, $D$ — число несогласованных пар.

\textbf{Доверительный интервал и тесты для корреляции.}
Для Пирсона при $H_0:\rho=0$ используют $t$-критерий
\[
t = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}} \sim t_{n-2},
\]
по которому строят двусторонний тест и доверительный интервал по схеме
“оценка $\pm t_{1-\alpha/2}\cdot \text{se}(r)$”.

Удобно также применять преобразование Фишера
\[
z = \tfrac12 \ln\frac{1+r}{1-r} = \operatorname{artanh}(r),
\]
для которого при большом $n$ выполняется
\[
z \approx \mathcal N\!\Bigl(\operatorname{artanh}(\rho),\, \frac{1}{n-3}\Bigr),
\]
а ДИ для $\rho$ получают обратным преобразованием через $\tanh$.

Для Спирмена и Кендалла используют либо перестановочные тесты,
либо асимптотическое приближение
\[
\frac{\hat\rho}{\text{se}(\hat\rho)} \approx \mathcal N(0,1),
\]
и дальше ту же стандартную схему “оценка $\pm$ квантиль $\times$ стандартная ошибка”.






\section{Корреляционная матрица; мультиколлинеарность; наведённая зависимость; частные корреляции; конкордация}
\textbf{Корреляционная матрица.} Симметрична, положительно полуопределённа; диагональ равна 1. Большие по модулю внедиагональные элементы — индикатор взаимосвязи признаков.

\textbf{Мультиколлинеарность.} 
\begin{itemize}
    \item Строгая коллинеарность: существует $v\neq 0$ с $Xv=0$ $\Rightarrow$ $X^\top X$ вырождена, оценки МНК неопределимы и зависимые предикторы надо удалить.
    \item Почти коллинеарность: $X^\top X$ плохо обусловлена $\Rightarrow$ дисперсии $\hat\beta$ раздуваются, знаки/величины неустойчивы, интерпретация ломается.
    \item Мультиколлинеарность — наличие сильной (почти линейной) зависимости между несколькими предикторами, когда один или несколько столбцов $X$ хорошо аппроксимируются линейной комбинацией остальных.
\end{itemize}


\textbf{Наведённая зависимость (конфаундинг).} 
Связь $X$ и $Y$ может объясняться общим фактором $Z$. Тогда маргинальная корреляция вводит в заблуждение; контролируем $Z$ и используем частные меры.

\textbf{Частная корреляция.} Очистка влияния $Z$:
\[
 \rho_{XY\cdot Z}= \frac{\rho_{XY}-\rho_{XZ}\rho_{YZ}}{\sqrt{(1-\rho_{XZ}^2)(1-\rho_{YZ}^2)}}.
\]

\textbf{Конкордация (согласованность ранжировок).} 
Коэффициент конкордации Кендалла $W$ агрегирует согласие нескольких ранжировок ("экспертов"). Для $m$ ранжировок $n$ объектов одна из формул:
\[
 W \;=\; \frac{12}{m^2(n^3-n)} \sum_{i=1}^n \left( \sum_{j=1}^m R_{ij} \;-
 \frac{m(n+1)}{2} \right)^2,
\]
где $R_{ij}$ — ранг $i$-го объекта у $j$-го эксперта. $W\in[0,1]$; большие значения означают сильное согласие. Также полезны условные/частные корреляции для интерпретации причинных структур.

\section{Повторные выборки; критерий знаков; Уилкоксона; ANOVA}
\textbf{Повторные (парные) выборки.} Сравниваем одну и ту же единицу до/после: анализируем разности $d_i=y^{(2)}_i-y^{(1)}_i$.

\textbf{Критерий знаков.} Тест медианы разностей $\mathrm{Med}(d_i)=0$. Статистика — число положительных знаков $B\sim\mathrm{Bin}(m,1/2)$, где $m$ — число ненулевых $d_i$. При больших $m$ — нормальная аппроксимация.

\textbf{Уилкоксона (ранговых знаков).} Ранжируем $|d_i|$, присваиваем знак, суммируем знаковые ранги $W$. При $n\gtrsim 10$ используется нормальная аппроксимация с поправкой на связи, для малых $n$ — точные таблицы. Более мощен, чем знаковый, при симметричных распределениях разностей.

\textbf{Однофакторная ANOVA.} Гипотеза $H_0: \mu_1=\dots=\mu_g$. Разложение вариации: $TSS=SS_B+SS_W$. Статистика
\[
 F=\frac{SS_B/(g-1)}{SS_W/(n-g)} \sim F_{g-1,\,n-g} \text{ при } H_0.
\]
Предпосылки: нормальность в группах, гомоскедастичность, независимость. Пост-хок сравнения: Tukey HSD, Bonferroni.

\section{МНК из ММП; нормальные уравнения}
\textbf{Линейная модель.} $y=X\beta+\varepsilon$, $\varepsilon\sim\mathcal N(0,\sigma^2 I)$.

\textbf{МНК как ММП.} Лог-правдоподобие:
\[
\ell(\beta,\sigma^2)= -\tfrac{n}{2}\ln(2\pi\sigma^2) - \tfrac{1}{2\sigma^2}\|y-X\beta\|^2.
\]
Максимизация по $\beta$ эквивалентна $\min\|y-X\beta\|^2$.

\textbf{Нормальные уравнения и оценки.} Нормальные уравнения: $X^\top X\,\hat\beta=X^\top y$, решение при $\operatorname{rank}(X)=k+1$:
\[
\hat\beta=(X^\top X)^{-1}X^\top y,\qquad \hat\sigma^2=RSS/n,\qquad RSS=\|y-X\hat\beta\|^2.
\]
Распределение: $\hat\beta\sim\mathcal N(\beta,\sigma^2( X^\top X)^{-1})$, а $\tfrac{RSS}{\sigma^2}\sim\chi^2_{n-k-1}$.

\section{Простая линейная регрессия; оценки, ДИ; Гаусс–Марков}
\textbf{Оценки коэффициентов.}
\[
\hat b_1=\frac{\overline{XY}-\overline X\,\overline Y}{\overline{X^2}-\overline X^{\,2}},\qquad \hat b_0=\overline Y-\hat b_1\overline X.
\]

\textbf{Дисперсии и доверительные интервалы.} 
\[
\widehat{\mathrm{Var}}(\hat b_1)=\hat\sigma^2/S_{xx},\qquad \widehat{\mathrm{Var}}(\hat b_0)=\hat\sigma^2\Bigl(\tfrac{1}{n}+\tfrac{\overline X^{2}}{S_{xx}}\Bigr),
\]
где $S_{xx}=\sum (x_i-\overline X)^2$. ДИ для среднего отклика и предсказания в $x_0$:
\[
\hat y_0\pm t_{\alpha/2,\,n-2}\,\hat\sigma\sqrt{\tfrac{1}{n}+\tfrac{(x_0-\overline X)^2}{S_{xx}}},\qquad
\hat y_0\pm t_{\alpha/2,\,n-2}\,\hat\sigma\sqrt{1+\tfrac{1}{n}+\tfrac{(x_0-\overline X)^2}{S_{xx}}}.
\]

\textbf{Теорема Гаусса–Маркова.} При линейности, экзогенности и гомоскедастичности МНК является BLUE (лучшей линейной несмещённой оценкой).

\section{Значимость предиктора/группы факторов}
\textbf{Значимость одного предиктора.} Один коэффициент: $t$-тест $t=\dfrac{\hat\beta_j-\beta_{j,0}}{\widehat{\mathrm{se}}(\hat\beta_j)}$.

\textbf{Значимость группы факторов.} Группа ограничений $R\beta=r$ (ранг $q$):
\[
F=\frac{(RSS_R-RSS_F)/q}{RSS_F/(n-k-1)} \sim F_{q,\,n-k-1},
\]
где $RSS_R$ и $RSS_F$ — остаточные суммы квадратов ограниченной и полной моделей. Эквивалентная форма через $R(\hat\beta-\beta)$ и ковариацию $\hat\beta$.

\section{Состоятельность МНК и асимптотическая нормальность}
\textbf{Состоятельность.} При $\tfrac{1}{n}X^\top X\to Q\succ 0$, $E(\varepsilon|X)=0$, $E(\varepsilon\varepsilon^\top|X)=\sigma^2 I$ и ограниченных моментах 
\[
\hat\beta \xrightarrow{p} \beta.
\]

\textbf{Асимптотическая нормальность.}
\[
\sqrt n(\hat\beta-\beta) \Rightarrow \mathcal N(0,\, \sigma^2 Q^{-1}).
\]
При гетероскедастичности асимптотическая нормальность сохраняется, но ковариацию заменяют на робастную (HC0–HC3).

\section{RSS, \(R^2\), \(R^2_{\text{adj}}\), AIC, BIC}
\textbf{RSS и коэффициент детерминации.}
\[
RSS=\sum r_i^2,\quad R^2=1-\frac{RSS}{TSS},\quad
R^2_{adj}=1-(1-R^2)\frac{n-1}{n-k-1}.
\]
\textbf{AIC и BIC.}
Для нормальной ошибки: $-2\ell(\hat\theta)= n\ln(RSS/n)+\text{const}$. Тогда
\[
AIC = n\ln(RSS/n)+2k,\qquad BIC = n\ln(RSS/n)+k\ln n
\]
(с точностью до константы). 
\textbf{Критерий $C_p$ Маллоуза.} $C_p=\tfrac{RSS}{\hat\sigma^2}-n+2k$.

\section{Ridge, Lasso, Elastic Net; байесовская мотивация}
\textbf{Ridge.} Ridge: $\min RSS+\lambda\|\beta\|_2^2$, решение $\hat\beta_{ridge}=(X^\top X+\lambda I)^{-1}X^\top y$. Стабилизирует при коллинеарности; требует стандартизации признаков.
\textbf{Lasso.} Lasso: $\min RSS+\lambda\|\beta\|_1$, даёт разреженные решения (координатный спуск/ISTA). 
\textbf{Elastic Net.} Elastic Net: $\lambda(\alpha\|\beta\|_1+(1-\alpha)\|\beta\|_2^2)$. Выбор $\lambda$ по $K$-fold CV. 
\textbf{Байесовская интерпретация.} Нормальное априори для Ridge, лапласовское для Lasso.

\section{Остатки: требования, стьюдентизированные остатки; нормальность}
\textbf{Требования к остаткам.} Линейность, экзогенность, гомоскедастичность, независимость, нормальность.

\textbf{Леверидж и стьюдентизированные остатки.} Леверидж $h_i=x_i^\top (X^\top X)^{-1}x_i$. Стандартизированные/стьюдентизированные остатки: $r_i/(\hat\sigma\sqrt{1-h_i})$.

\textbf{Диагностика и нормальность.} Диагностика по графикам Residuals vs Fitted, QQ-plot, Scale–Location. Тесты нормальности: Джарка–Бера, Шапиро–Уилка.

\textbf{Влияние наблюдений.} Расстояние Кука $D_i=\tfrac{r_i^2}{p\hat\sigma^2}\tfrac{h_i}{(1-h_i)^2}$.

\section{Автокорреляция остатков: признаки и тесты}
\textbf{Признаки автокорреляции.} ACF/PACF остатков, график $e_t$ vs $e_{t-1}$.

\textbf{Тест Дарбина–Уотсона.} $DW\approx 2(1-\hat\rho_1)$ для проверки первой автокорреляции.

\textbf{Тест Бреуша–Годфри.} Регрессия $e_t$ на лаги $e_{t-1..p}$ и $X$; LM-статистика $\sim\chi^2_p$.

\textbf{Тест Льюнга–Бокса.} 
\[
Q= n(n+2)\sum_{h=1}^H \hat\rho_h^2/(n-h)\sim\chi^2_H.
\]

\section{Гетероскедастичность: признаки и тесты}
\textbf{Признаки гетероскедастичности.} Графики Scale–Location и Residuals vs Fitted с веерообразным рисунком.

\textbf{Тест Бреуша–Пагана.} Регрессия $e_i^2$ на $X$, LM-статистика $\sim\chi^2_k$.

\textbf{White-тест.} Регрессия дисперсии с квадратичными и перекрёстными членами предикторов.

\textbf{Робастные ковариации.} Используют HC0–HC3 для корректировки ковариационной матрицы оценок.

\section{Преобразования Бокса–Кокса и Йео–Джонсона}
\textbf{Преобразование Бокса–Кокса.}
Box–Cox (для $y>0$): $g_{\lambda}(y)=\tfrac{y^{\lambda}-1}{\lambda}$ при $\lambda\neq 0$, и $\ln y$ при $\lambda=0$. 
\textbf{Преобразование Йео–Джонсона.}
Yeo–Johnson допускает $y\le 0$.
\textbf{Практика.} (i) подбор $\lambda$ по максимуму $\ell(\lambda)$; (ii) трансформировать $Y$ и/или $X$, переоценить модель и проверить гомоскедастичность; (iii) можно выбирать $\lambda$ по минимуму $\log|\widehat{\Sigma}|$ по предикторам.

\section{Подбор предикторов: forward/backward, ADD–Del}
\textbf{Forward.} Старт с константы, добавляем лучший по AIC/BIC/$R^2_{adj}$/CV; стоп при отсутствии улучшения.

\textbf{Backward.} Старт с полной модели, удаляем наихудший предиктор.

\textbf{ADD–Del.} После каждого добавления пытаемся удалить любой уже включённый признак по тому же критерию.

Избегать утечки (фиксировать валидацию).

\section{Бинарные переменные, one-hot; взаимодействия}
\textbf{One-hot кодирование.} $k$ уровней $\Rightarrow$ $k-1$ дамми плюс базовая категория (иначе ловушка фиктивных переменных). Коэффициент при дамми — сдвиг относительно базы.

\textbf{Взаимодействия.} Взаимодействие $X\times D$ задаёт разные наклоны между группами; интерпретируем через комбинации базовых эффектов.

\section{Нефиксированные потери: LAD, Huber, Tukey, LMS; LAD из ММП}
\textbf{LAD (L1).} Минимизирует $\sum |e_i|$, ММП при лапласовских ошибках.

\textbf{Huber.} $\rho_c(e)=\begin{cases} e^2/2,&|e|\le c\\ c|e|-c^2/2,&|e|>c \end{cases}$, даёт линейную $\psi$-функцию.

\textbf{Потеря Тьюки (biweight).} Ограничивает влияние больших $|e|$.

\textbf{LMS.} Минимум медианы $e_i^2$; крайне робастно, но неэффективно.

Оценивание: IRLS/координатный спуск.

\section{Сравнение LS/LAD/LMS; Пуассон-регрессия}
\textbf{LS.} Оптимален при нормальных ошибках, чувствителен к выбросам.

\textbf{LAD.} Более устойчив к выбросам в $y$, даёт медианный фит.

\textbf{LMS.} Обеспечивает максимальную устойчивость, но имеет высокую дисперсию и дорог в вычислениях.

\textbf{Пуассон-регрессия.} Пуассон GLM: $Y\sim\mathrm{Poisson}(\mu)$, линк $\log\mu=X\beta$, $\mathrm{Var}(Y)=\mu$. Девианс 
\[
D=2\sum\bigl[y_i\ln(y_i/\hat\mu_i)-(y_i-\hat\mu_i)\bigr].
\]

\section{Латентные переменные; EM-алгоритм}
\textbf{Латентные переменные.} Ненаблюдаемые $Z$, которые влияют на распределение наблюдаемых $Y$ и входят в полное правдоподобие.

\textbf{EM-алгоритм.} EM максимизирует $\ell(\theta)$ при латентных $Z$. E-шаг: 
\[
Q(\theta|\theta^{(t)})=E_{Z|Y,\theta^{(t)}}[\ell_c(\theta)].
\]
M-шаг: $\theta^{(t+1)}=\arg\max Q(\theta|\theta^{(t)})$. Для смеси норм: веса $\gamma_{ik}$ и обновления $\pi_k,\mu_k,\Sigma_k$. Свойство: монотонный рост $\ell(\theta^{(t)})$.

\section{Временные ряды: определения; сезонность; стационарность; ADF}
\textbf{Белый шум.} $e_t$ некоррелирован, $E(e_t)=0$, $\mathrm{Var}(e_t)=\sigma^2$.

\textbf{Случайное блуждание.} $y_t=y_{t-1}+e_t$.

\textbf{MA(q).} $y_t=\sum_{i=0}^q \theta_i e_{t-i}$.

\textbf{AR(p).} $y_t=\sum_{i=1}^p \phi_i y_{t-i}+e_t$.

\textbf{Сезонность.} Аддитивная/мультипликативная структура, повторяющаяся с периодом.

\textbf{ADF-тест.} Регрессия 
\[
\Delta y_t=\alpha+\beta t+\gamma y_{t-1}+\sum_{i=1}^p \psi_i \Delta y_{t-i}+u_t,
\]
тест $H_0: \gamma=0$ на наличие единичного корня.

\section{AR, MA, ARMA, ARIMA: уравнения, оценивание, проверка}
\textbf{ARIMA(p,d,q).} $\nabla^d y_t$ моделируется ARMA(p,q).

\textbf{Идентификация порядка.} Анализ ACF/PACF, информкритерии AIC/BIC.

\textbf{Оценивание.} (Квази-)ММП или уравнения Йюла–Уокера для AR-части.

\textbf{Проверка модели.} Анализ остатков по ACF/PACF и тесту Льюнга–Бокса.

\section{Регрессия с временными рядами; детрендирование; Гаусс–Марков для рядов}
\textbf{Детрендирование и сезонность.} Удаляем тренд и вводим сезонные дамми-переменные перед оцениванием регрессии.

\textbf{ARIMAX/GLS.} При авторегрессии ошибок используют ARIMAX или GLS вместо обычного МНК.

\textbf{Робастная ковариация HAC.} Оценка Newey–West даёт корректные $t/F$-статистики при условной гетероскедастичности и слабой зависимости.

\textbf{Гаусс–Марков для рядов.} Классические BLUE не работают без некоррелированности ошибок, поэтому применяют GLS/Cohrane–Orcutt.





\end{document}